{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c92e01-9239-4a9e-8bd1-f0f21273add9",
   "metadata": {
    "id": "e9c92e01-9239-4a9e-8bd1-f0f21273add9"
   },
   "source": [
    "# Text Summarization with BART (Hugging Face Transformers)\n",
    "\n",
    "This notebook applies a pretrained **BART model** (`facebook/bart-base`) from Hugging Face to perform **abstractive text summarization**.  \n",
    "\n",
    "**Key steps:**\n",
    "1. Load dataset (`train.json`) and (`test.json`) with text-summary pairs.  \n",
    "2. Preprocess input text.  \n",
    "3. Use Hugging Face `pipeline` for summarization with `facebook/bart-base`.  \n",
    "4. Generate summaries for sample texts.  \n",
    "5. Compare generated summaries with references qualitatively.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6194b212-161f-40b4-bdc7-2dca93ee7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Import PyTorch and related libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6e227-5aaf-421c-8138-22399bdaf797",
   "metadata": {},
   "source": [
    "- Install pytorch_lightning (if not installed)\n",
    "- Run: !pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707b413e-18de-4d6e-8180-6f55ecec8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Hugging Face Transformers\n",
    "from transformers import(\n",
    "AutoTokenizer,\n",
    "AutoModelForSeq2SeqLM,\n",
    "get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "#Import PyTorch Lightning for simplified training\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969c7bf8-7b28-4c3b-bd68-ab2a7f2a70b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#Set random seeds for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "#Constants\n",
    "MAX_LEN = 150\n",
    "SUMMARY_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"facebook/bart-base\"  #Using BART which is good for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709cb82-542b-47c2-b1f5-37f2f637f559",
   "metadata": {
    "id": "d709cb82-542b-47c2-b1f5-37f2f637f559"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- Source: DeepLearning.AI  \n",
    "- Format: JSON (`train.json`) and(`test.json`) with fields:\n",
    "  - `dialogue`: original passage  \n",
    "  - `summary`: human-written reference summary  \n",
    "\n",
    "Example:\n",
    "```json\n",
    "{\n",
    "  \"dialogue\": \"Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him ðŸ™‚ Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\",\n",
    "  \"summary\": \"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d64177-1f33-48e4-9861-935b26eb8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and preprocess data\n",
    "def load_data(data_dir):\n",
    "    train_data = pd.read_json(f\"{data_dir}/train.json\")\n",
    "    test_data = pd.read_json(f\"{data_dir}/test.json\")\n",
    "    return train_data, test_data\n",
    "\n",
    "#Dataset class\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, summary_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.summary_len = summary_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        document = str(self.data.iloc[index]['dialogue'])\n",
    "        summary = str(self.data.iloc[index]['summary'])\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            document,\n",
    "            max_length = self.max_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length = self.summary_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e4b5f-18fe-4814-88d0-4b978b756df4",
   "metadata": {
    "id": "462e4b5f-18fe-4814-88d0-4b978b756df4"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Model: facebook/bart-base\n",
    "\n",
    "We use the Hugging Face `transformers` library with the **BART model**, a pretrained sequence-to-sequence transformer fine-tuned for summarization tasks.\n",
    "\n",
    "Steps:\n",
    "- Load `facebook/bart-base` tokenizer and model.  \n",
    "- Build a summarization `pipeline`.  \n",
    "- Generate summaries for sample texts.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1068b0-c6d1-499b-ba8e-3f14e3fc474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightning Module for training\n",
    "class SummaryModel(pl.LightningModule):\n",
    "    def __init__(self, model_name = MODEL_NAME, lr = LEARNING_RATE):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels= None):\n",
    "        outputs = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch['input_ids'],\n",
    "            batch['attention_mask'],\n",
    "            batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch['input_ids'],\n",
    "            batch['attention_mask'],\n",
    "            batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('val_loss', loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96d17b-43ec-4cd5-a86c-ec7855af332d",
   "metadata": {
    "id": "8d96d17b-43ec-4cd5-a86c-ec7855af332d"
   },
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394dc84e-ca99-47fb-acb0-78be1556d36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cad56199bde43858b6315a96d1f5660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cd2570e2e84fc5b2a71705e062aa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3031ef6eed244530b1c3e11dcd257e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db6b8d5bce44781bcf9554a4dacc738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "3#Load data\n",
    "data_dir = \"data/corpus\"\n",
    "train_data, test_data = load_data(data_dir)\n",
    "\n",
    "#Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#Create datsets\n",
    "train_dataset = SummaryDataset(train_data, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "val_dataset = SummaryDataset(test_data, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "#Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6afb3c-5610-49c1-8043-ee14f5ae06e5",
   "metadata": {
    "id": "2d6afb3c-5610-49c1-8043-ee14f5ae06e5"
   },
   "source": [
    "#### Instantiating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d90c6df-3d1b-455d-94b7-559311c07f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d7981976ae4c1ebaafe92f2c199c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type                         | Params | Mode\n",
      "--------------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 139 M  | eval\n",
      "--------------------------------------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "557.682   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "182       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4be477c9a514f20b6c609e16fc44e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7656bd9b5b4062b5bb3553aefb264c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c863bccff194afeb9d98a9608909c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36651fb5512e4b77a8dbdf3a498b8455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47128958dd124ad2bb2356c89e28aa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bf779203124e1fadcf01a4cd01589e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92049cade413439cadc58a6643a72de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "#Initialize model\n",
    "model = SummaryModel()\n",
    "\n",
    "#Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = 'val_loss',\n",
    "    dirpath = 'checkpoints',\n",
    "    filename = 'best-checkpoint',\n",
    "    save_top_k = 1,\n",
    "    mode = 'min'\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name = \"summarization\")\n",
    "\n",
    "#Determinee devices based on availability\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "    devices = 1\n",
    "else:\n",
    "    accelerator = 'cpu'\n",
    "    devices = 'auto'\n",
    "\n",
    "#Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = EPOCHS,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    accelerator = 'auto',\n",
    "    devices = devices\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "#Example summarization\n",
    "example_text = train_data.iloc[0]['dialogue']\n",
    "inputs = tokenizer(\n",
    "    example_text,\n",
    "    max_length = MAX_LEN,\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67c9c7-3e64-45c6-b37a-01a9027240b0",
   "metadata": {
    "id": "8d67c9c7-3e64-45c6-b37a-01a9027240b0"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Instead of BLEU/ROUGE, we perform **qualitative evaluation** by comparing predicted summaries against human-written references.\n",
    "\n",
    "Example:\n",
    "- Input: \"The stock market crashed yesterday due to global uncertainty...\"  \n",
    "- Predicted: \"Global uncertainty caused a market crash.\"  \n",
    "- Reference: \"Stock market crashed due to uncertainty.\"  \n",
    "\n",
    "Observation:\n",
    "- The model captures the main idea but may paraphrase differently.  \n",
    "- Summaries are concise and fluent.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408a8c46-b9b3-4126-a6b6-027f2beb2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "My major priority is to validate the usage of our material for organ-on-a-chip applications, guided by market analysis and customer discovery. Given David's background in pharmacy, I am especially keen to have him identify specific pathologies, treatments, and drug R&D groups that would be a good fit, so that we can tailor our technical development pathway and funding strategy. He's already made a good start on a market opportunity analysis document and I expect that the bulk of his efforts over the next few weeks will be focused on customer validation/customer discovery. The document should have been shared with you recently- and I am happy to do the same with subsequent documents related to the project.\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      "My priority is to validate the usage of our material for organ-on-a-chip applications, guided by market analysis and customer discovery. David has a background in pharmacy. David will work on a market opportunity analysis document.\n"
     ]
    }
   ],
   "source": [
    "#Load best model\n",
    "example_text = \"\"\"My major priority is to validate the usage of our material for organ-on-a-chip applications, guided by market analysis and customer discovery. Given David's background in pharmacy, I am especially keen to have him identify specific pathologies, treatments, and drug R&D groups that would be a good fit, so that we can tailor our technical development pathway and funding strategy. He's already made a good start on a market opportunity analysis document and I expect that the bulk of his efforts over the next few weeks will be focused on customer validation/customer discovery. The document should have been shared with you recently- and I am happy to do the same with subsequent documents related to the project.\n",
    "\"\"\"\n",
    "inputs = tokenizer(\n",
    "    example_text,\n",
    "    max_length = MAX_LEN,\n",
    "    truncation = True,\n",
    "    padding = 'max_length',\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "\n",
    "best_model = SummaryModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "best_model.eval()\n",
    "\n",
    "device = next(best_model.parameters()).device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#Generate summary\n",
    "summary_ids = best_model.model.generate(\n",
    "    input_ids = inputs['input_ids'],\n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_length = SUMMARY_LEN,\n",
    "    num_beams = 2,\n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    summary_ids[0],\n",
    "    skip_special_tokens = True,\n",
    "    clean_up_tokenization_spaces = True\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Text:\")\n",
    "print(example_text)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16b886-e342-4c87-959d-9dd841217e7c",
   "metadata": {
    "id": "de16b886-e342-4c87-959d-9dd841217e7c"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "- Successfully applied `facebook/bart-base` for abstractive summarization.\n",
    "- Model outputs are fluent and concise, closely matching human references.\n",
    "\n",
    "---\n",
    "#### Text summarization is highly useful for real-world applications such as:\n",
    "- Condensing news articles into short briefs.\n",
    "- Summarizing legal or research documents for quicker understanding.\n",
    "- Providing quick insights from long customer support logs.\n",
    "##### This project demonstrates how transformer-based models like BART can deliver immediate value in reducing information overload by generating high-quality summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MiB9UuiwPc_2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
